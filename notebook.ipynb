{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahed/hive-IR/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load MedQuAD XML data\n",
    "chunks = []\n",
    "data_root = Path(\"/Users/mahed/VsProjects/MedQuAD\")  # Adjust if needed\n",
    "\n",
    "for subfolder in data_root.glob(\"*_QA\"):\n",
    "    for file in subfolder.glob(\"*.xml\"):\n",
    "        try:\n",
    "            tree = ET.parse(file)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            q_el = root.find(\".//Question\")\n",
    "            a_el = root.find(\".//Answer\")\n",
    "\n",
    "            if q_el is not None and a_el is not None:\n",
    "                question = q_el.text.strip() if q_el.text else \"\"\n",
    "                answer = a_el.text.strip() if a_el.text else \"\"\n",
    "\n",
    "                if question and answer:\n",
    "                    chunks.append({\n",
    "                        \"text\": f\"Q: {question}\\nA: {answer}\",\n",
    "                        \"meta\": {\n",
    "                            \"source\": subfolder.name,\n",
    "                            \"file\": file.name\n",
    "                        }\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5394\n",
      "{'text': 'Q: What is (are) keratoderma with woolly hair ?\\nA: Keratoderma with woolly hair is a group of related conditions that affect the skin and hair and in many cases increase the risk of potentially life-threatening heart problems. People with these conditions have hair that is unusually coarse, dry, fine, and tightly curled. In some cases, the hair is also sparse. The woolly hair texture typically affects only scalp hair and is present from birth. Starting early in life, affected individuals also develop palmoplantar keratoderma, a condition that causes skin on the palms of the hands and the soles of the feet to become thick, scaly, and calloused.  Cardiomyopathy, which is a disease of the heart muscle, is a life-threatening health problem that can develop in people with keratoderma with woolly hair. Unlike the other features of this condition, signs and symptoms of cardiomyopathy may not appear until adolescence or later. Complications of cardiomyopathy can include an abnormal heartbeat (arrhythmia), heart failure, and sudden death.  Keratoderma with woolly hair comprises several related conditions with overlapping signs and symptoms. Researchers have recently proposed classifying keratoderma with woolly hair into four types, based on the underlying genetic cause. Type I, also known as Naxos disease, is characterized by palmoplantar keratoderma, woolly hair, and a form of cardiomyopathy called arrhythmogenic right ventricular cardiomyopathy (ARVC). Type II, also known as Carvajal syndrome, has hair and skin abnormalities similar to type I but features a different form of cardiomyopathy, called dilated left ventricular cardiomyopathy. Type III also has signs and symptoms similar to those of type I, including ARVC, although the hair and skin abnormalities are often milder. Type IV is characterized by palmoplantar keratoderma and woolly and sparse hair, as well as abnormal fingernails and toenails. Type IV does not appear to cause cardiomyopathy.', 'meta': {'source': '3_GHR_QA', 'file': '0000559.xml'}}\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks))\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load embedding models\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Encode MedQuAD text with CLIP\n",
    "clip_text_embeddings = []\n",
    "for chunk in chunks:\n",
    "    inputs = clip_processor(text=[chunk[\"text\"]], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    embedding = clip_model.get_text_features(**inputs).detach().cpu().numpy()[0]\n",
    "    clip_text_embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_text_embeddings = np.array(clip_text_embeddings)\n",
    "text_index = faiss.IndexFlatL2(clip_text_embeddings.shape[1])\n",
    "text_index.add(clip_text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load DermNet dataset and embed sample images\n",
    "image_chunks = []\n",
    "image_embeddings = []\n",
    "dermnet = load_dataset(\"dermnet\", split=\"train\")\n",
    "\n",
    "for i in range(100):  # Limit for speed/test\n",
    "    item = dermnet[i]\n",
    "    image = item[\"image\"]\n",
    "    label = item[\"label\"]\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    embedding = clip_model.get_image_features(**inputs).detach().cpu().numpy()[0]\n",
    "    image_chunks.append({\"image\": image, \"label\": label})\n",
    "    image_embeddings.append(embedding)\n",
    "\n",
    "image_embeddings = np.array(image_embeddings)\n",
    "image_index = faiss.IndexFlatL2(image_embeddings.shape[1])\n",
    "image_index.add(image_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
